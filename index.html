<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="418project : Website for 15418 final project">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>418project</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/abutko/418project">View on GitHub</a>

          <h1 id="project_title">418 project: Parallel Hash Tables</h1>
          <h2 id="project_tagline">Website for 15418 final project</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/abutko/project418/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/abutko/project418/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Parallel Hash Tables - Final Report</h3>

<a href = "proposal.html">Proposal </a><br>

<a href = "checkpoint.html">Checkpoint Report </a>

<p><strong>TITLE.</strong> Parallel Hash Tables. Brought to you by Manu Garg, Andrew Butko.</p>

<p><strong>SUMMARY.</strong> We implemented concurrent hash tables that use coarse grained/fine grained locks. We also implemented
a lock-free version of hash tables. Given the speedup of our implementations we demonstrate that lock-free is faster than fine-grained
which in turn is faster than coarse-grained in most scenarios.</p>

<p><strong>BACKGROUND.</strong> A hashtable is a datastructure that maps keys to values and allows fast accesses to the data.  
More on hash tables: <a href="https://en.wikipedia.org/wiki/Hash_table">hash tables</a>. The operations on a hashtable include 
searching a key, inserting a key, value pair and deleting a key from the table. We implemented the hashtable with separate chaining 
i.e. a linked list per bucket (Fig.1)

<figure>
  <img src="hashtable.png" alt="hash table">
  <figcaption> Fig.1 - Hashtable with separate chaining</figcaption>
</figure><br>

Although none of the individual operations are computationaly expensive (roughly O(1) average access time per op for a good hash table 
implementation), they are highly parallelizable, especially when contention isn't high because at that point the different operations
are accessing different buckets and you can perform them in parallel without any dependencies.
</p>

<p><strong>APPROACH.</strong></p>

<p>Coarse-grained: In this implementation we used a global pthread read-write lock for the table. On insert/delete we acquire a
write lock on the table and then we insert/remove the key, value pair in the corresponding hashed bucket 
whereas in search we acquired a read lock on the table and then we try to find the entry in the corresponding hashed bucket. 
This allows multiple readers to read the table but only one writer
for the table. On resize which happens in insert when the load factor hits a certain threshold we acquire a write lock on the table and
then create a new bucket array and copy over the contents of the original table, rehashing them to their new locations. This approach 
could potentially lead to starvation since it provides no fairness guarantees </p>

<p>Fine-grained: In this implementation we used an array of pthread read-write locks, one per bucket of the table. Although this
approach allows parallelization at bucket level as well, we found it to be slower than using mutex locks, one per bucket of the table. 
This is because we expect O(1) access time at the bucket level and the overhead in the acquisiting of a pthread read-write lock 
destroyed any potential gain obtained from parallelization a single bucket level. This approach could also lead to starvation.</p>

<p>Lock-free: </p>

<p>Each operation uses a common function called search. This function takes in a search key and return references to two nodes left node,
right node for that key (left node's key < search key < right node's key) and both nodes are unmarked which means that they haven't been
deleted from the table (checked through a mark bit in each node's next pointer). Finally the right node must be the immediate successor
of the left node. This function is divided into three phases. The first phase iterates across the corresponding bucket to find the first 
unmarked node whose key >= search key. This is our right node. The first phase also finds the last unmarked left node. The second phase 
checks if the right node is the immediate successor of the left node, if that is the case and the right node isn't marked then
we return else we call search again. The last phase uses an atomic CAS operation to remove all the unmarked nodes between left node and right node.</p>

<p>put: After finding the corresponding bucket where the key should be stored, we call search on the bucket to get the left node and 
the right node in between which the key should be inserted. If the key is already stored in the bucket (i.e. in the right node) then 
we do an atomic exchange of the old value with the new value. Otherwise we do a single atomic CAS operation to insert a new node with 
the input key, value pair in between the left node and the right node</p>

<p>remove: After finding the corresponding bucket where the key should be removed from, we call search on the bucket to locate the node 
to delete and then use a two-phase process to perform the deletion. First, the node is logically deleted by marking the reference 
in the node's next field. Next, the node is physically deleted by attempting a single atomic CAS operation to swing left node's next
to right node's next, if this doesn't work then the deletion takes places within an invocation of search (It's 3rd phase does the trick) </p>

<p>get: After finding the corresponding bucket where the key should be located, we call search on the bucket to get the node where 
the key might be present.</p>

<p>For a more detailed explanation of the above mentioned operations refer to the code on github or the 
<a href="https://timharris.uk/papers/2001-disc.pdf">research paper</a> on which this implementation is based on.</p>

<p>
<table class="projectSchedule">
<tr>
  <td width="110"><span style="font-weight: bold;">Week</span></td>
  <td width="380"><span style="font-weight: bold;">Goals</span></td>
  <td width="380"><span style="font-weight: bold;">Assigned To</span></td>
  <td width="380"><span style="font-weight: bold;">Completed?</span></td>

</tr>
<tr>
  <td>April 3-9</td>
  <td>Read listed papers and research RMC model. Plan interface and testing suite. </td>
  <td>Andrew/Manu</td>
  <td>Yes</td>
</tr>

<tr>
  <td>April 10-16</td>
  <td>
    Implement fine and coarse locking schemes.
  </td>
  <td>Andrew/Manu</td>
  <td>Yes</td>
</tr>

<tr>
  <td>April 17-20</td>
  <td>
    Test various parameters on completed implementations, start to compile testing results.
  </td>
  <td>Manu</td>
  <td></td>
</tr>
<tr>
  <td>April 20-23</td>
  <td>
    Re-read paper on lock-free version and start to code.
  </td>
  <td>Andrew</td>
  <td></td>
</tr>
<tr>
  <td>April 24-27</td>
  <td>
    Finish coding lock-free implementation and performance analysis.
  </td>
  <td>Andrew/Manu</td>
  <td></td>
</tr>
  <td>April 27-30</td>
  <td>
    Buffer days.
  </td>
  <td>Andrew/Manu</td>
  <td></td>
</tr>

<tr>
  <td>May 1-4</td>
  <td>
    BDD if we are ahead. Begin to compile final report.
  </td>
  <td>Andrew/Manu</td>
  <td></td>
</tr>

<tr>
  <td>May 5-8</td>
  <td>
    Final buffer days, complete final report.
  </td>
  <td>Andrew/Manu</td>
  <td></td>
</tr>
</table>
</p>

<p><strong>WORK COMPLETED.</strong> 
We have completed the implementation of fine and coarse grain locking schemes 
(these are available in this repository under the code/ directory). Coarse-grain is implemented with a single pthread reader-writer lock 
on the table while fine-grain adds mutex locks for each bucket in the hash table. We also designed a testing suite that
allows us to vary parameters such as workload size, operation distribution, and the range of keys. This is currently done
pseudo-randomly, and we will explore variations on this in the future. Note - we started with a generic sequential code base
from https://gist.github.com/aozturk/2368896.
</p>
<p><strong>GOALS AND DELIVERABLES.</strong> 
We will definitely be able to complete all deliverables previously mentioned (lock-free implementation, performance analysis).
For the competition we would show performance graphs.
We are on track with our initial goals and don't have anything new to add. We listed BDDs under the "nice to haves"
and have left room in the schedule to work on this if we complete our primary goals on time. We have not researched this thoroughly
and are not sure of the amount of work this will involve, but with the remaining time we have we hopefully will manage to
have some additional features.
</p>

<p><strong>PRELIMINARY RESULTS</strong>
With our testing suite we have seen a performance improvement with the fine-grain locking as expected. We tested this with
std::mutex and pthread reader-writer locks and found mutex locks had better performance. As we just completed the testing harness,
we still need to compile an intial performance report (speedup graphs). This is the first goal listed on the updated 
schedule above.
</p>
  </body>
</html>

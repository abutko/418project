<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="418project : Website for 15418 final project">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>418project</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/abutko/418project">View on GitHub</a>

          <h1 id="project_title">418 project: Parallel Hash Tables</h1>
          <h2 id="project_tagline">Website for 15418 final project</h2>
            <section class="page-header">
              <a id="proposal" href="proposal.html" class="btn">Proposal</a>
              <a id="checkpoint" href="checkpoint.html" class="btn">Checkpoint Report</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Parallel Hash Tables - Final Report</h3>

<p>By Manu Garg, Andrew Butko</p>

<p><strong>SUMMARY.</strong></p>

<p>We implemented concurrent hash tables that use coarse grained/fine grained locks. We also implemented
a lock-free version of hash tables. Given the speedup of our implementations we demonstrate that lock-free is faster than fine-grained
which in turn is faster than coarse-grained in most scenarios.</p>

<p><strong>BACKGROUND.</strong></p> 

<p>A hashtable is a datastructure that maps keys to values and allows fast accesses to the data.  
More on hash tables: <a href="https://en.wikipedia.org/wiki/Hash_table">hash tables</a>. The operations on a hashtable include 
searching a key, inserting a key, value pair and deleting a key from the table. We implemented the hashtable with separate chaining 
i.e. a linked list per bucket (Fig.1)

<figure>
  <img src="hashtable.png" alt="hash table">
  <figcaption><center>Fig.1 - Hashtable with separate chaining</center></figcaption>
</figure><br>

Although none of the individual operations are computationaly expensive (roughly O(1) average access time per op for a good hash table 
implementation), they are highly parallelizable, especially when contention isn't high because at that point the different operations
are accessing different buckets and you can perform them in parallel without any dependencies.
</p>

<p><strong>APPROACH.</strong></p>

<p>We used c++ for writing the code and GHC machines for testing. The operations in each of the following implementations can be 
called in parallel by separate threads. We started with a basic working <a href=https://gist.github.com/aozturk/2368896>sequential version</a> 
and built upon that further to make different parallelized versions of the same.</p>

<p><u>Coarse-grained:</u> In this implementation we used a global pthread read-write lock for the table. On insert/delete we acquire a
write lock on the table and then we insert/remove the key, value pair in the corresponding hashed bucket 
whereas in search we acquired a read lock on the table and then we try to find the entry in the corresponding hashed bucket. 
This allows multiple readers to read the table but only one writer
for the table. On resize which happens in insert when the load factor hits a certain threshold we acquire a write lock on the table and
then create a new bucket array and copy over the contents of the original table, rehashing them to their new locations. This approach 
could potentially lead to starvation since it provides no fairness guarantees </p>

<p><u>Fine-grained:</u> In this implementation we used an array of pthread read-write locks, one per bucket of the table. Although this
approach allows parallelization at bucket level as well, we found it to be slower than using mutex locks, one per bucket of the table. 
This is because we expect O(1) access time at the bucket level and the overhead in the acquisiting of a pthread read-write lock 
destroyed any potential gain obtained from parallelization a single bucket level. This approach could also lead to starvation.</p>

<p><u>Lock-free:</u></p>

<p>Each operation uses a common function called search. This function takes in a search key and return references to two nodes left node,
right node for that key (left node's key < search key < right node's key) and both nodes are unmarked which means that they haven't been
deleted from the table (checked through a mark bit in each node's next pointer). Finally the right node must be the immediate successor
of the left node. This function is divided into three phases. The first phase iterates across the corresponding bucket to find the first 
unmarked node whose key >= search key. This is our right node. The first phase also finds the last unmarked left node. The second phase 
checks if the right node is the immediate successor of the left node, if that is the case and the right node isn't marked then
we return else we call search again. The last phase uses an atomic CAS operation to remove all the unmarked nodes between left node and right node.</p>

<p><u>put:</u> After finding the corresponding bucket where the key should be stored, we call search on the bucket to get the left node and 
the right node in between which the key should be inserted. If the key is already stored in the bucket (i.e. in the right node) then 
we do an atomic exchange of the old value with the new value. Otherwise we do a single atomic CAS operation to insert a new node with 
the input key, value pair in between the left node and the right node</p>

<p><u>remove:</u> After finding the corresponding bucket where the key should be removed from, we call search on the bucket to locate the node 
to delete and then use a two-phase process to perform the deletion. First, the node is logically deleted by marking the reference 
in the node's next field. Next, the node is physically deleted by attempting a single atomic CAS operation to swing left node's next
to right node's next, if this doesn't work then the deletion takes places within an invocation of search (It's 3rd phase does the trick) </p>

<p><u>get:</u> After finding the corresponding bucket where the key should be located, we call search on the bucket to get the node where 
the key might be present.</p>

<p><u>Lock-free Limitations:</u> Although the remove operation removes the key from the hashtable, it doesn't delete the node. This won't 
be an issue if C++ had a garbage collector, but without one the code above leaks memory. A fix to that would be to use hazard 
pointers which we tried to implement and use but weren't successful in doing so given our situation and the lack of time. Another 
limitation of lock-free is that we don't resize, this won't be an issue if we expect the number of removes to be of the order of the 
number of inserts.</p>

<p>For a more detailed version of the above mentioned operations refer to the 
<a href=https://github.com/abutko/418project/tree/gh-pages/code>code</a> on github or the 
<a href="https://timharris.uk/papers/2001-disc.pdf">research paper</a> which the lock-free implementation is based on.</p>

<p><strong>RESULTS</strong>
Our testing methodology is based on the performance evaluation in Maged Michael's paper "High Performance Dynamic Lock-Free Hash Tables
and List-Based Sets" (listed below). 
We test the effects of varying the number threads, the range of keys used, the initial size of the table, and the ratio of different operations used on performance measured as average CPU time per operation.
In every experiment, we fix the number of threads. Each thread performs 1 million operations chosen randomly with a predetermined ratio of put/delete/get operations.
Before measuring these operations, we prepopulate the hash table based on a chosen load factor. For load factor <strong>a</strong> and table size <strong>m</strong>, we initially add <strong>a * m </strong> elements and restrict the range of elements from 0 to <strong>2am</strong>.
This was chosen based on the results Michael published to have a benchmark reference.
</p>
<p>
  <div id="parent" width="1000">
  <img src="images/avg11.jpg" alt="a11" height="300">      
  <img src="images/Michael_11.png" alt="m11" height="300"><br>
  <div style="clear:both"></div>
</div>
</p>

<p><strong>REFERENCES.</strong></p>

<p><a href="https://timharris.uk/papers/2001-disc.pdf">https://timharris.uk/papers/2001-disc.pdf</a></p>

<p><a href="http://www.research.ibm.com/people/m/michael/spaa-2002.pdf">http://www.research.ibm.com/people/m/michael/spaa-2002.pdf</a></p>

<p><a href="http://www.drdobbs.com/lock-free-data-structures-with-hazard-po/184401890">http://www.drdobbs.com/lock-free-data-structures-with-hazard-po/184401890</a></p>

<p><a href="https://gist.github.com/aozturk/2368896">https://gist.github.com/aozturk/2368896</a></p>

<p><a href="https://en.wikipedia.org/wiki/Hash_table">https://en.wikipedia.org/wiki/Hash_table</a></p>

<p><strong>WORK BY EACH STUDENT.</strong></p>

<p>Equal work was performed by both project members.</p>
  </body>
</html>
